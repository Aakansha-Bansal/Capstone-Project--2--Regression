{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aakansha-Bansal/Capstone-Project--2--Regression/blob/main/Bike_sharing_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Seoul Bike Sharing Demand Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - **Regression**\n",
        "##### **Contribution**    - **Individual**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this dataset there are total of 8760 rows and 14 columns.\n",
        "\n",
        "### Count of Rented Bike : Dependent variable\n",
        "\n",
        "### Normally distributed attributes: - Temperature - Humidity.\n",
        "\n",
        "### Positively correlated variables to the bike rent are : - Temperature - Dew Point Temperature - Solar Radiation - Hour\n",
        "\n",
        "### Negatively correlated variables are: - Humidity - Rainfall - Weekdays or Weekends\n",
        "\n",
        "### The number of bikes rented is on average higher during the rush hours.i.e. at 6 p.m. to 8 p.m.\n",
        "\n",
        "### The rented bike counts is higher during the summer and lowest during the winter.\n",
        "\n",
        "### The rented bike count is higher on working days than on non-working days.\n",
        "\n",
        "### On a non-functioning day, no bikes are rented in all the instances of the data.\n",
        "\n",
        "### The number of bikes rented on average remains constant throughout Monday - Saturday, it dips on Sunday, and on average, the rented bike counts is lower on weekends than on weekdays.\n",
        "\n",
        "### On regular days, the demand for the bikes is higher during rush hours. On holidays or weekends, the demand is comparatively lower in the mornings, and is higher in the afternoons.\n",
        "\n",
        "### After doing EDA I have implemented 7 machine learning algorithms i.e., Linear Regression,lasso,ridge,elasticnet,decission tree, Random Forest and XGBoost. I have done hyperparameter tuning to improve the model performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Aakansha-Bansal/Capstone-Project--2--Regression"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UtJA5l3GZi8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "bike_df=pd.read_csv('/content/drive/MyDrive/SeoulBikeData.csv',encoding ='latin')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_df"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head()"
      ],
      "metadata": {
        "id": "COdNMfFGcIWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "bike_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(bike_df[bike_df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The dataset given is a dataset from Bike rental company, and we have to analysis the data of customers and the insights behind it.\n",
        "\n",
        "### The above dataset has 8760 rows and 14 columns. Thankfully, in this dataset there are neither has duplicate values nor has null values.\n",
        "\n",
        "### In a day we have 24 hours and we have 365 days in a year so 365*24 = 8760, which represents the number of line in the dataset which means we have the data of whole year.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the column names\n",
        "### Some of the columns name in the dataset are too large and clumsy so we change that names into some simple names, and it don't affect our end results."
      ],
      "metadata": {
        "id": "84UhCFkWYYko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "0N75zc2KYRsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe().T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Date** : The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str, we need to convert into datetime format.\n",
        "\n",
        "### **Rented_Bike_Count** : Number of rented bikes per hour which our dependent variable and we need to predict that, type : int\n",
        "\n",
        "### **Hour**: The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.\n",
        "\n",
        "### **Temperature**: Temperature in Celsius, type : Float\n",
        "\n",
        "### **Humidity** : Humidity in the air in %, type : int\n",
        "\n",
        "### **Wind speed** : Speed of the wind in m/s, type : Float\n",
        "\n",
        "### **Visibility** : Visibility in m, type : int\n",
        "\n",
        "### **Dew point temperature** : Temperature at the beggining of the day, type : Float\n",
        "\n",
        "### **Solar Radiation** : Sun contribution, type : Float\n",
        "\n",
        "### **Rainfall** : Amount of raining in mm, type : Float\n",
        "\n",
        "### **Snowfall** : Amount of snowing in cm, type : Float\n",
        "\n",
        "### **Seasons** : Season of the year, type : str, there are only 4 season's in data .\n",
        "\n",
        "### **Holiday** : If the day is holiday period or not, type: str\n",
        "\n",
        "### **Functioning Day** : If the day is a Functioning Day or not, type : str"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "bike_df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python read \"Date\" column as a object type basically it reads as a string, as the date column is very important to analyze the users behaviour so we need to convert it into datetime format then we split it into 3 column i.e 'year', 'month', 'day'as a category data type.*"
      ],
      "metadata": {
        "id": "IpHyNdMii0Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "bike_df['Date'] = bike_df['Date'].apply(lambda x:  dt.datetime.strptime(x,\"%d/%m/%Y\"))"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['year'] = bike_df['Date'].dt.year\n",
        "bike_df['month'] = bike_df['Date'].dt.month\n",
        "bike_df['day'] = bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "KROdzUTljC9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "bike_df['weekdays_weekend']=bike_df['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "bike_df=bike_df.drop(columns=['Date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "L8iRic_sjICw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\".\n",
        "### The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november so if i consider this is a one year then we don't need the \"year\" column so we drop it.\n",
        "### The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df"
      ],
      "metadata": {
        "id": "uhtDtft0j9cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head()"
      ],
      "metadata": {
        "id": "eGH2mV9kkJ9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['weekdays_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "s1-1YZLnkh5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually it is a category data type. so we need to change this data type if we not then, while doing the further anlysis and correleted with this then the values are not actually true so we can mislead by this."
      ],
      "metadata": {
        "id": "kuIEAlfBkJsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into catagory column\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "  bike_df[col]=bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "mLe5C3Xtk44s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's check the result of data type\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "uFClJygwlAsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.columns"
      ],
      "metadata": {
        "id": "Tbr-zQq6g2-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Which month experiences more bikes on rent?"
      ],
      "metadata": {
        "id": "7pXCjHZnlaDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.barplot(data=bike_df,x='month',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count_of_Rented_bikes_acording_to_month ')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bar charts show the frequency counts of values for the different levels of a categorical or nominal variable."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can clearly conclude from graph that summer season is the busiest season for rented bikes and june is most demanded season."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This plot provides positive insight that summer months are more likely by customers."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On which days more bikes has been rented?"
      ],
      "metadata": {
        "id": "0_c9V1RUnIdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=bike_df,x='weekdays_weekend',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekenday ')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As this chart gives proper visulization about on which days more demand of bikes have seen."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the above bar plot we can say that in the week days which represent in blue colour shows that the demand of the bikes are higher in weekdays."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The insight is positive that weekdays are more likely the busy days for bike demand obviously because of office(work) days."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On Which time the demand is high?"
      ],
      "metadata": {
        "id": "k6uFhb7gVGAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekend ')"
      ],
      "metadata": {
        "id": "EfU7QfU-WQ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As line chart gives the proper comparison between weekdays and weekend according to time."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Peak Time are 7 am to 9 am and 5 pm to 7 pm.The orange colur represents the weekend days, and it show that the demand of rented bikes are very low specially in the morning hour but when the evening start from 4 pm to 8 pm the demand slightly increases."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The gained insights are positive as it gives us the idea that on which time we have to advertise so that more customers will attract."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demand of bikes is more on Functioning Day or on Non-Functioning Day?"
      ],
      "metadata": {
        "id": "gY9rzB7FZeKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=bike_df,x='Functioning_Day',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This chart gives the proper count of customers."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The above bar plot shows the use of rented bike in functioning day or not, and we can clearly see that People don't use reneted bikes in no functioning day or we can say that people use rented bikes on functioning days only."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The insight is that functioning day are busy days for rented bikes."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which season is most busy season for bike demand?"
      ],
      "metadata": {
        "id": "yH1TfoNNcAUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.barplot(data=bike_df,x='Seasons',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Seasons ')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As this chart provides clear comparison among seasons."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The bar plot shows the use of rented bike in four different seasons, and it clearly seen that in summer season the use of rented bike is high and peak time is 7am-9am and 7pm-5pm and in winter season the use of rented bike is very low."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We got the insight that the most demanding season is Summer followed by Autumn, Spring and Winter."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seasons"
      ],
      "metadata": {
        "id": "WwRaPl2rHvcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Seasons',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to seasons ')"
      ],
      "metadata": {
        "id": "OJ44bqxpH1RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "32jGQMX7ICDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  The point plot  shows the use of rented bike count in four different seasons."
      ],
      "metadata": {
        "id": "r_fLQpBpIL3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tEaFNPevIZSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In summer season the use of rented bike is high and peak time is 7am-9am and 5pm-7pm\n"
      ],
      "metadata": {
        "id": "Pf4h-p90Ihzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help creating a positive business impact?.\n",
        "\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "9nTbULmdIhwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summer season is most demanding season for rented bikes and winter season   is very low demanded season because of snowfall.\n"
      ],
      "metadata": {
        "id": "m1VX6pfcI5Jt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 7"
      ],
      "metadata": {
        "id": "AYArIz3UJT7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Whether the demand of bike is more on holidays or on normal days?"
      ],
      "metadata": {
        "id": "0hG4x96becZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "fig,ax=plt.subplots(figsize=(8,8))\n",
        "sns.barplot(data=bike_df,x='Holiday',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As this chart gives the proper comparison of customers on Holiday and No Holiday."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The above bar plot shows the use of rented bike in a holiday, and it is clearly shown in chart that on normal days more people uses the rented bike than on holiday."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yes, this chart provides the  positive insight that the demand of rented bikes are more on No Holiday days."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bikes count per hour"
      ],
      "metadata": {
        "id": "nKmaRsaAcicV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.barplot(data=bike_df,x='Hour',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Hour ')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As this chart gives the proper counts of bikes per hour."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The above plot shows the use of rented bike according the hours and the data are from all over the year generally people use rented bikes during their working hour from 7am to 9am and 5pm to 7pm. 6 p.m. is the most demanding time for rented bikes."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The insight is that working hours are most busy hours for rented bike."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### On Which day rented bikes demand is more on Functioning Day or on Non-Functioning Day?"
      ],
      "metadata": {
        "id": "GJ4chGfFdaR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Functioning_Day',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As this chart provides clear result about the situation."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This plot shows the use of rented bikes on functioning day or not and it clearly shown that People don't use reneted bikes in no functioning day."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The insight is that people use rented bikes on functioning days only."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analyze Numerical variables**"
      ],
      "metadata": {
        "id": "ld6_JTJshoMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is numerical data?**\n",
        " ### Numerical data is a data type expressed in numbers rather than in language description. Numerical data ia always collected in number form."
      ],
      "metadata": {
        "id": "dal-EDKLIcWm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze of Numerical variables distplots"
      ],
      "metadata": {
        "id": "2uY3D8iwinvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "numerical_columns=list(bike_df.select_dtypes(['int64','float64']).columns)\n",
        "numerical_features=pd.Index(numerical_columns)\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_features:\n",
        "  plt.figure(figsize=(10,6))\n",
        "  sns.distplot(x=bike_df[col])\n",
        "  plt.xlabel(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lsnWDwUie3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical vs.Rented_Bike_Count"
      ],
      "metadata": {
        "id": "0RM1dmtBi5GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relationship between \"Rented_Bike_Count\" and \"Temperature\" \n",
        "bike_df.groupby('Temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "LM2Nd65ei8t3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By this graph we can see that when Temperature increases the demand of rented bikes is also increasing. From the above plot we see that people like to ride bikes when it is pretty hot around 25°C average."
      ],
      "metadata": {
        "id": "vG9EmljUjGBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relationship between \"Rented_Bike_Count\" and \"Dew_point_temperature\" \n",
        "bike_df.groupby('Dew_point_temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "lS2-X8yLjthG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By this plot we can see dew point is almost same as temperature."
      ],
      "metadata": {
        "id": "5uB0KgW9KNJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relationship between \"Rented_Bike_count\" and \"Solar_Radiation\"\n",
        "bike_df.groupby('Solar_Radiation').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "f9-QjquKSKaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the above plot we see that, the amount of rented bikes is huge, when there is solar radiation, the counter of rents is around 1000.\n"
      ],
      "metadata": {
        "id": "suplkmugSlfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relationship between \"Rented_Bike_Count\" and \"Snowfall\" \n",
        "bike_df.groupby('Snowfall').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "wNYta3GGkIzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This graph clearly represents that when snowfall increases demand of rented bikes decreases."
      ],
      "metadata": {
        "id": "rdoOr7Z5kW4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relationship between \"Rented_Bike_Count\" and \"Rainfall\" \n",
        "bike_df.groupby('Rainfall').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "vqbgFyThknd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can see from the above plot that even if it rains a lot the demand of of rent bikes is not decreasing, here for example even if we have 20 mm of rain there is a big peak of rented bikes.\n"
      ],
      "metadata": {
        "id": "5cnOw8sx_e4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relationship between \"Rented_Bike_Count\" and \"Wind_speed\" \n",
        "bike_df.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "RJs68Fook51M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By this plot we can see demand of rented bikes is uniformly distributed but when speed of wind was 7 m/s the count of rented bikes increased. This means that people like to ride bike when it is little windy."
      ],
      "metadata": {
        "id": "cKtTJdV3K8mN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression plot**"
      ],
      "metadata": {
        "id": "J8MHkARalVmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The regression plots in seaborn are primarily used to add a visual guide that helps to emphasize patterns in a dataset during EDA. Regression plots means as its name suggests that creates a regression line between 2 parameters and helps to visualize their linear relationships."
      ],
      "metadata": {
        "id": "WR__WKkmMwPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the regression plot for all the numerical features\n",
        "for col in numerical_features:\n",
        "  fig,ax=plt.subplots(figsize=(10,6))\n",
        "  sns.regplot(x=bike_df[col],y=bike_df['Rented_Bike_Count'],scatter_kws={\"color\": 'orange'}, line_kws={\"color\": \"black\"})"
      ],
      "metadata": {
        "id": "22DGxgEMlbL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the above regression plot of all numerical features we see that the columns Temperature, Wind_speed, Solar_Radiation Snowfall are positively relation to the target variable.which means the rented bike count increases with increase of these features.\n",
        "### Rainfall, Snowfall, Humidity these features are negatively related with the target variable which means the rented bike count decreases when these features increase."
      ],
      "metadata": {
        "id": "C6rlqLKAAF4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thankfully, there is no missing values in this dataset."
      ],
      "metadata": {
        "id": "M2akcqaJfjR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalise Rented_Bike_Count column"
      ],
      "metadata": {
        "id": "EYzPcYhoPK29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The data normalization (also referred to as data pre-processing) is a basic element of data mining. It means transforming the data, namely converting the source data in to another format that allows processing data effectively. The main purpose of data normalization is to minimize or even exclude duplicated data."
      ],
      "metadata": {
        "id": "ipA3qSR1NsZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "#Distribution plot of Rented Bike Count\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.xlabel('Rented_Bike_Count')\n",
        "plt.ylabel('Density')\n",
        "ax=sns.distplot(bike_df['Rented_Bike_Count'],hist=True ,color=\"y\")\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The above graph shows that Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform some operation to make it normal.\n"
      ],
      "metadata": {
        "id": "Iv7spd0HN3E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#Boxplot of Rented Bike Count to check outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=bike_df['Rented_Bike_Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The above boxplot shows that we have to detect outliers in Rented Bike Count column.\n"
      ],
      "metadata": {
        "id": "gtJ7Fg6VOAcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying square root to Rented Bike Count to improve skewness\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.distplot(np.sqrt(bike_df['Rented_Bike_Count']), color=\"y\")\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fZJ8oTzam7Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Since we have generic rule of applying Square root for the skewed variable in order to make it normal .After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution.\n"
      ],
      "metadata": {
        "id": "0Pnd2j-POKbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After applying sqrt on Rented Bike Count check wheater we still have outliers \n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=np.sqrt(bike_df['Rented_Bike_Count']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G8ghPbF3mWRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After applying Square root to the Rented Bike Count column, we find that there is no outliers present and I have used the square root transformation for normalising the skewness."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature selection**\n",
        "**Checking OLS model**\n",
        "\n",
        "### Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable."
      ],
      "metadata": {
        "id": "y_JPkwMiAe17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the module\n",
        "#assign the 'x','y' value\n",
        "import statsmodels.api as sm\n",
        "x = bike_df[[ 'Temperature','Humidity',\n",
        "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
        "       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n",
        "y = bike_df['Rented_Bike_Count']"
      ],
      "metadata": {
        "id": "3X-9GXzvAecL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add a constant column\n",
        "x = sm.add_constant(x)   # here we are adding b0\n",
        "\n",
        "#fit a OLS model \n",
        "model= sm.OLS(y,x).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "9joiqneBA6Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### R sqauare and Adj Square are near to each other. 40% of variance in the Rented Bike count is explained by the model.\n",
        "\n",
        "### For F statistic , P value is less than 0.05 for 5% levelof significance.\n",
        "\n",
        "### P value of dew point temp and visibility are very high and they are not significant.\n",
        "\n",
        "### Omnibus tests the skewness and kurtosis of the residuals. Here the value of Omnibus is high., it shows we have skewness in our data.\n",
        "\n",
        "### The condition number is large, 3.11e+04. This might indicate that there are strong multicollinearity or other numerical problems\n",
        "\n",
        "### Durbin-Watson tests for autocorrelation of the residuals. Here value is less than 0.5. We can say that there exists a positive auto correlation among the variables."
      ],
      "metadata": {
        "id": "fA5lyfenT8Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Correlation Map**"
      ],
      "metadata": {
        "id": "WiZv0_36Bjzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation map\n",
        "plt.figure(figsize=(16,15))\n",
        "corr = bike_df.corr()\n",
        "sns.heatmap(corr,annot=True,fmt='.2f',annot_kws={'size':15})\n",
        "plt.title('correlation between features');"
      ],
      "metadata": {
        "id": "Oy1xjyLyBo_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can observe on the heatmap that on the target variable line the most positively correlated variables to the bike rent are :\n",
        "### the temperature\n",
        "### the dew point temperature\n",
        "### the solar radiation\n",
        "\n",
        "### And most negatively correlated variables are:\n",
        "### Humidity\n",
        "### Rainfall\n",
        "### weekdays or weekends\n",
        "\n",
        "### From above correlation map, we can see that, there is high correlation between 'Dew Point Temperature' and 'Temperature'. Here we are featuring the best suitable model,so we have to drop either one of the feature i.e. either Temperature or Dew Point Temperature.\n"
      ],
      "metadata": {
        "id": "YrFyPp1kBxgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping dew point temperature column\n",
        "bike_df = bike_df.drop(['Dew_point_temperature','Visibility'],axis=1)"
      ],
      "metadata": {
        "id": "6UMAowMwCVN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "pKGmz1CACd6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Data preprocessing**\n"
      ],
      "metadata": {
        "id": "3Ra9JgBzCr1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Create the dummy variables**\n",
        "### A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables.\n"
      ],
      "metadata": {
        "id": "CdjOkDIdrvQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = bike_df.select_dtypes(['object','category']).columns\n",
        "categorical_features    "
      ],
      "metadata": {
        "id": "IdND-KcVDEaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **One hot encoding**\n",
        "### A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical."
      ],
      "metadata": {
        "id": "yR-TcZQ1sSnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creat a copy\n",
        "bike_df_copy = bike_df\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "    data = data.drop([column], axis=1)\n",
        "    return data\n",
        "\n",
        "for col in categorical_features:\n",
        "    bike_df_copy = one_hot_encoding(bike_df_copy, col)\n",
        "bike_df_copy.head()       "
      ],
      "metadata": {
        "id": "-hOTcf88s7ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n"
      ],
      "metadata": {
        "id": "Ti8bZvTAteP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Test split for regression"
      ],
      "metadata": {
        "id": "LLycIYCtxmoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the value in X and Y\n",
        "X = bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(bike_df_copy['Rented_Bike_Count'])"
      ],
      "metadata": {
        "id": "nMEE5naTt0Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "ZVK5s2DhuBOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "gF0JNx4rusb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As of now, we have assign the independent variables = x and dependent variable = y."
      ],
      "metadata": {
        "id": "5GYMYJLuuIHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "hfnzOPjFrZr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "kG83bdBErh44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I have used 75% data for training and 25% data for testing because to avoid overfitting."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.describe().columns"
      ],
      "metadata": {
        "id": "L8nao0F7qJ6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them.\n",
        "It’s called the mean squared error as you’re finding the average of a set of errors. The lower the MSE, the better the forecast.\n",
        "\n",
        "MSE formula = (1/n) * Σ(actual – forecast)2\n",
        "Where:\n",
        "\n",
        "n = number of items,\n",
        "\n",
        "Σ = summation notation,\n",
        "\n",
        "Actual = original or observed y-value,\n",
        "\n",
        "Forecast = y-value from regression.\n",
        "\n",
        "Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors).\n",
        "\n",
        "Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. ... Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable.\n",
        "\n",
        "R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
        "\n",
        "Formula for R-Squared\n",
        "\n",
        "      R^2 = 1- Unexplained Variation/Total Variation\n",
        "\n",
        "\n",
        "R\n",
        "2 =1− Total Variation Unexplained Variation​\n",
        "\n",
        "Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        "​\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "G6jsmN5Nqy9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LINEAR REGRESSION**\n",
        "\n",
        "Regression models describe the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line\n",
        "\n",
        "Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independent variables and dependent variable. In case of single variable, the formula is same as straight line equation having an intercept and slope.\n",
        "\n",
        "     y_pred = b0+b1x\n",
        "\n",
        "where b0 and b1 are intercept and slope respectively.\n",
        "\n",
        "In case of multiple features the formula translates into:\n",
        " y_pred = b0 + b1x1 + b2x2 + b3x3 + ....\n",
        "where x_1,x_2,x_3 are the features values and b0,b1,b2...are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.\n",
        "\n",
        "Gradient descent is the process by which the algorithm tries to update the parameters using a loss function . Loss function is nothing but the diffence between the actual values and predicted values(aka error or residuals). There are different types of loss function but this is the simplest one. Loss function summed over all observation gives the cost functions. The role of gradient descent is to update the parameters till the cost function is minimized i.e., a global minima is reached. It uses a hyperparameter 'alpha' that gives a weightage to the cost function and decides on how big the steps to take. Alpha is called as the learning rate. It is always necesarry to keep an optimal value of alpha as high and low values of alpha might make the gradient descent overshoot or get stuck at a local minima. There are also some basic assumptions that must be fulfilled before implementing this algorithm. They are:\n",
        "\n",
        "1.No multicollinearity in the dataset.\n",
        "\n",
        "2.Independent variables should show linear relationship with dv.\n",
        "\n",
        "3.Residual mean should be 0 or close to 0.\n",
        "\n",
        "4.There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.\n",
        "\n",
        "Let us now implement our first model. We will be using LinearRegression from scikit library.\n",
        "\n"
      ],
      "metadata": {
        "id": "_JTNWQ62rjpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Regression**"
      ],
      "metadata": {
        "id": "3S4zieczR4sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "LinearRegression\n",
        "# Fit the Algorithm\n",
        "reg= LinearRegression().fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the coefficeint\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "ZUcocCZMtlI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)"
      ],
      "metadata": {
        "id": "wBYOS07-tuF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "dH6SxCxLt2sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looks like our r2 score value is 0.77 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons."
      ],
      "metadata": {
        "id": "pyJS8PgguK9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "5X0AwG_8uT_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score((y_test), (y_pred_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",Adjusted_R2_lr )"
      ],
      "metadata": {
        "id": "Brmvof-suXp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The r2_score for the test set is 0.78. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter)."
      ],
      "metadata": {
        "id": "Fe-1z_L9ulmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "test_df=pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "_S9r9MQ_uuX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test))"
      ],
      "metadata": {
        "id": "acrPh_VDuwyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xz99a9xgu4UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LASSO REGRESSION**"
      ],
      "metadata": {
        "id": "G31IxsF2vCRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of Lasso Regression implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X_train, y_train)\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ],
      "metadata": {
        "id": "lEk8qgDCIdHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "PZ-h1-nrIkmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "a7i-Os57IrRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Looks like our r2 score value is 0.40 that means our model is not able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "qVzJTALHvsjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2)\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "_hjEroUGv9CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "VroUpWnMv_Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The r2_score for the test set is 0.38. This means our linear model is not performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "SRH0e2tMwHyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "1wrUDo6-Qubo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ntb5yoSCwRxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))"
      ],
      "metadata": {
        "id": "KugxNDXVwaeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "r9zXRtZcKlkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RIDGE REGRESSION**"
      ],
      "metadata": {
        "id": "ldqGNtTGwiiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "ridge= Ridge(alpha=0.1)\n",
        "# Fit the Algorithm\n",
        "ridge.fit(X_train,y_train)\n",
        "# Predict on the model\n",
        "ridge.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "8NCpYUxow2xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "_Q9gWyRJw324"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Looks like our r2 score value is 0.77 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "LV16JO8RxFPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "Lu9cSC8oxKMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "E1w5kL6PxOX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The r2_score for the test set is 0.78. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "q5-2BydwxVas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "xyfutrJzRCb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot((y_pred_test_ridge))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VUZO8QZ4xb1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge))"
      ],
      "metadata": {
        "id": "ZPx4uBkGxiAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "jej112s6x4JR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ELASTIC NET REGRESSION**"
      ],
      "metadata": {
        "id": "cL0xf3RyyMAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a * L1 + b * L2\n",
        "#alpha = a + b and l1_ratio = a / (a + b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "#FIT THE MODEL\n",
        "elasticnet.fit(X_train,y_train)\n",
        "#check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "a5lil2gHyPAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_en=elasticnet.predict(X_train)\n",
        "y_pred_test_en=elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "I-wQubJeyclz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "6BZiIqBFyfCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Looks like our r2 score value is 0.62 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "jk0AuLEnyoXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Elastic net regression ',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "kqZlVvjzyrhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_e= mean_squared_error(y_test, y_pred_test_en)\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_test, y_pred_test_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score((y_test), (y_pred_test_en))\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "mfFrNWEdywMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The r2_score for the test set is 0.62. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "MIp3npsNy98u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Elastic net regression Test',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "uLdUKnAWRU-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_test_en))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hsH-FohMzH_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_en),(y_test)-(y_pred_test_en))"
      ],
      "metadata": {
        "id": "o1P4C0JIzNhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5"
      ],
      "metadata": {
        "id": "q10taTvO1L05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **DECISION TREE**"
      ],
      "metadata": {
        "id": "ItuuuzUi1D33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 5 Implementation\n",
        "decision_regressor = DecisionTreeRegressor(criterion='squared_error', max_depth=8,max_features=9, max_leaf_nodes=100,)\n",
        "decision_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NKAGXoMq1Sdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_d = decision_regressor.predict(X_train)\n",
        "y_pred_test_d = decision_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "kjnD876H2VYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Score:\",decision_regressor.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_d= mean_squared_error(y_train, y_pred_train_d)\n",
        "print(\"MSE :\",MSE_d)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE :\",RMSE_d)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_d= mean_absolute_error(y_train, y_pred_train_d)\n",
        "print(\"MAE :\",MAE_d)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_d= r2_score(y_train, y_pred_train_d)\n",
        "print(\"R2 :\",r2_d)\n",
        "Adjusted_R2_d=(1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "biUVUmJW2UCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Looks like our r2 score value is 0.71 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "XKQYbHcp3mMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Dicision tree regression ',\n",
        "       'MAE':round((MAE_d),3),\n",
        "       'MSE':round((MSE_d),3),\n",
        "       'RMSE':round((RMSE_d),3),\n",
        "       'R2_score':round((r2_d),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "TRmD1ePL3vhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_d= mean_squared_error(y_test, y_pred_test_d)\n",
        "print(\"MSE :\",MSE_d)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE :\",RMSE_d)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_d= mean_absolute_error(y_test, y_pred_test_d)\n",
        "print(\"MAE :\",MAE_d)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_d= r2_score((y_test), (y_pred_test_d))\n",
        "print(\"R2 :\",r2_d)\n",
        "Adjusted_R2_d=(1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "o2zTHtSI3z5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The r2_score for the test set is 0.68. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter)**."
      ],
      "metadata": {
        "id": "C2QRAch73_RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Dicision tree regression ',\n",
        "       'MAE':round((MAE_d),3),\n",
        "       'MSE':round((MSE_d),3),\n",
        "       'RMSE':round((RMSE_d),3),\n",
        "       'R2_score':round((r2_d),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "rlftvoZQSElw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot((np.array(y_pred_test_d)))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tBEXZ-P24NU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_d),(y_test)-(y_pred_test_d))"
      ],
      "metadata": {
        "id": "bWPagu3c4Tqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6"
      ],
      "metadata": {
        "id": "Q8CDSjq04qYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RANDOM FOREST**"
      ],
      "metadata": {
        "id": "OdacaVgJ4uB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "QYHlSIIu4zG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "y_pred_train_r = rf_model.predict(X_train)\n",
        "y_pred_test_r = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "VS-PeQEi4-eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Score:\",rf_model.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_train, y_pred_train_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_train, y_pred_train_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score(y_train, y_pred_train_r)\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "e87HQ-Wp5F_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Looks like our r2 score value is 0.98 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "TbHeOVdK5K5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "dGEvFnTB5TrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_test, y_pred_test_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_test, y_pred_test_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score((y_test), (y_pred_test_r))\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "3eFrWrWM5XlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The r2_score for the test set is 0.91. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "ayyBbZVF5hCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "Q_5IlD0hScTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_r),(y_test)-(y_pred_test_r))"
      ],
      "metadata": {
        "id": "8aB8lOtM5o-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.feature_importances_"
      ],
      "metadata": {
        "id": "ivb79cDC5wCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "yXDOBkau5xzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "hkJ5Md6D51u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "XZ59T3uT55rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Ov9eX2bp6BiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "NSxOOOIW6SeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wa2Uptbw6Vqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here, we can see that, Temperature, Functioning Day and Humidity has much higher relevance on the counting of bike Renting. means they are most important features which affects on rented bike count in the Random Forest model.\n",
        "\n"
      ],
      "metadata": {
        "id": "CQrBvA9wMCLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 7"
      ],
      "metadata": {
        "id": "h2rCZnE06dxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRADIENT BOOSTING**"
      ],
      "metadata": {
        "id": "M7hfChAO6mey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "#Fit the model\n",
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "s2z-inMf6pGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "y_pred_train_g = gb_model.predict(X_train)\n",
        "y_pred_test_g = gb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "DntpP6QU6zbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Score:\",gb_model.score(X_train,y_train))\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_train, y_pred_train_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_train, y_pred_train_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score(y_train, y_pred_train_g)\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "oaOLKHzB64j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Looks like our r2 score value is 0.87 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "kdEJ3tjs7Ret"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "UZpbD9l87V28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_test, y_pred_test_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_test, y_pred_test_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score((y_test), (y_pred_test_g))\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "HyZS8oYI7aXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The r2_score for the test set is 0.86. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "SAr5EBd57j0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "7JT8w2wgUbQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_g),(y_test)-(y_pred_test_g))"
      ],
      "metadata": {
        "id": "_2hB3mR57txK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.feature_importances_"
      ],
      "metadata": {
        "id": "15-Wqmkm75W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "1IujHVs477zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "YN_f-EVC8EH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "-YnO-SWw8JMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "TeYuWcMn8O2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "hhmEVjc28jnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "RlxzcDhf83rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z5MgzA4v8533"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "UEbeKg0Z9imU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before proceding to try next models, let us try to tune some hyperparameters and see if the performance of our model improves.\n",
        "\n",
        "### Hyperparameter tuning is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithms is hyperparameter tuning.\n",
        "\n",
        "### **Using GridSearchCV**\n",
        "\n",
        "### GridSearchCV helps to loop through predefined hyperparameters and fit the model on the training set. So, in the end, we can select the best parameters from the listed hyperparameters.\n"
      ],
      "metadata": {
        "id": "eGllETtI9mZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Boosting Regressor with GridSearchCV**"
      ],
      "metadata": {
        "id": "STGb4IcJ-L2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Provide the range of values for chosen hyperparameters**"
      ],
      "metadata": {
        "id": "5wujEHJy-IwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "RH-HzbFE-QrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict"
      ],
      "metadata": {
        "id": "8cOJYZIU-T0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Gradient Boosting Regressor**"
      ],
      "metadata": {
        "id": "n64wFoxv-awY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Grid search\n",
        "gb_grid = GridSearchCV(estimator=gb_model,\n",
        "                       param_grid = param_dict,\n",
        "                       cv = 5, verbose=2)\n",
        "\n",
        "gb_grid.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "GrWrsfRC-dfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "gKXnPPZkACv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model = gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "MICndijXAJhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_params_"
      ],
      "metadata": {
        "id": "KMqQt279AMcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_g_g = gb_optimal_model.predict(X_train)\n",
        "y_pred_g_g= gb_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "6dDhJIpFAURB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Score:\",gb_optimal_model.score(X_train,y_train))\n",
        "MSE_gbh= mean_squared_error(y_train, y_pred_train_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_train, y_pred_train_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score(y_train, y_pred_train_g_g)\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "tvYlUMWWAVe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "dvkQWWCtAb_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE_gbh= mean_squared_error(y_test, y_pred_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_test, y_pred_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score((y_test), (y_pred_g_g))\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_test, y_pred_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_g_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "uH8Vs_dVAgMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "KJ8VDDCQXd1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_g_g),(y_test)-(y_pred_g_g))"
      ],
      "metadata": {
        "id": "nWZMBRLHAogY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model.feature_importances_"
      ],
      "metadata": {
        "id": "4TkhwrBEAyf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_optimal_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)\n",
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n",
        "importance_df.head()"
      ],
      "metadata": {
        "id": "2QUUu_TcA2f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "mmfTlD5eA_He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "R23LMyzZBi-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "BXGZP_jbBo0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kNvqzuVCBtX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **XgBoost with RandomizedSearchCV**"
      ],
      "metadata": {
        "id": "7_08F4i2Yw0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Library\n",
        "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
        "import xgboost"
      ],
      "metadata": {
        "id": "GHYlaADsY6dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameter optimization\n",
        "params = {'learning_rate' : [0.05,0.1,0.15,0.2,0.25,0.3],\n",
        "          'max_depth' : [3,4,5,6,8,10,12,15],\n",
        "          'min_child_weight' : [1,3,5,7],\n",
        "          'gamma' : [0,0.1,0.2,0.3,0.4],\n",
        "          'colsample_bytree' : [0.3,0.4,0.5,0.7]\n",
        "} "
      ],
      "metadata": {
        "id": "R71uF6_3Y8hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_regressor = xgboost.XGBRegressor()\n",
        "random_search = RandomizedSearchCV(xgb_regressor, param_distributions=params, n_iter=5, scoring='neg_mean_squared_error',n_jobs=-1,cv=5,verbose=3)"
      ],
      "metadata": {
        "id": "vO9HotqDZCV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Vx867gtKZGwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = random_search.best_estimator_\n",
        "xgb_model"
      ],
      "metadata": {
        "id": "ChUxgBITZTkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search.best_params_"
      ],
      "metadata": {
        "id": "YA7kxXw5ZVx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the y predicted value for train and test dataset\n",
        "y_pred_train = xgb_model.predict(X_train)\n",
        "y_pred_test = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "-xGrVtPLZfHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating metric value for testing dataset\n",
        "print('Metric value of training dataset')\n",
        "#Calculate MSE\n",
        "MSE_xgb = mean_squared_error(y_train, y_pred_train)    \n",
        "print(\"MSE :\",MSE_xgb)\n",
        "#calculate RMSE \n",
        "RMSE_xgb = np.sqrt(MSE_xgb)   \n",
        "print(\"RMSE :\",RMSE_xgb)\n",
        "#calculate MAE \n",
        "MAE_xgb = mean_absolute_error(y_train, y_pred_train)       \n",
        "print(\"MAE :\",MAE_xgb)\n",
        "#calculate r2 and adjusted r2\n",
        "r2_xgb = r2_score(y_train, y_pred_train)         \n",
        "print(\"R2 :\",r2_xgb)\n",
        "Adjusted_r2_xgb = 1-(((1-r2_xgb)*(X_test.shape[0]-1))/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 :\",Adjusted_r2_xgb)\n",
        "#creating table of metric values\n",
        "dict1={'Model':'XgBoost Regressor with RandomisedSearchCV',\n",
        "       'MAE':round((MAE_xgb),3),\n",
        "       'MSE':round((MSE_xgb),3),\n",
        "       'RMSE':round((RMSE_xgb),3),\n",
        "       'R2_score':round((r2_xgb),3),\n",
        "       'Adjusted R2':round((Adjusted_r2_xgb),3)\n",
        "        }\n",
        "training_df = training_df.append(dict1,ignore_index=True)\n",
        "\n",
        "#Calculating metric value for testing dataset\n",
        "#calculate MSE\n",
        "print('Metric value of testing dataset')\n",
        "MSE_xgb = mean_squared_error(y_test, y_pred_test)    \n",
        "print(\"MSE :\",MSE_xgb)\n",
        "#calculate RMSE \n",
        "RMSE_xgb = np.sqrt(MSE_xgb)   \n",
        "print(\"RMSE :\",RMSE_xgb)\n",
        "#calculate MAE \n",
        "MAE_xgb = mean_absolute_error(y_test, y_pred_test)       \n",
        "print(\"MAE :\",MAE_xgb)\n",
        "#calculate r2 and adjusted r2\n",
        "r2_xgb = r2_score(y_test, y_pred_test)         \n",
        "print(\"R2 :\",r2_xgb)\n",
        "Adjusted_r2_xgb = 1-(((1-r2_xgb)*(X_test.shape[0]-1))/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "print(\"Adjusted R2 :\",Adjusted_r2_xgb)\n",
        "#creating table of metric values\n",
        "dict2={'Model':'XgBoost Regressor with RandomisedSearchCV',\n",
        "       'MAE':round((MAE_xgb),3),\n",
        "       'MSE':round((MSE_xgb),3),\n",
        "       'RMSE':round((RMSE_xgb),3),\n",
        "       'R2_score':round((r2_xgb),3),\n",
        "       'Adjusted R2':round((Adjusted_r2_xgb),3)\n",
        "        }\n",
        "test_df = test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "Ea6EZ1FaZjmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XgBoost with RandomisedSearchCV Model\n",
        "### R2 score of Train data : 0.96\n",
        "### adjusted R2 score of Train data : 0.96\n",
        "### R2 score of Test data : 0.92\n",
        "### adjusted R2 score of Test data : 0.92"
      ],
      "metadata": {
        "id": "bhocqKPzbMJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSE is the most popular evaluation metric used in regression problems. It follows an assumption that error are unbiased and follow a normal distribution. Here are the key points to consider on RMSE:\n",
        "\n",
        "### The power of ‘square root’ empowers this metric to show large number deviations. The ‘squared’ nature of this metric helps to deliver more robust results which prevents cancelling the positive and negative error values. In other words, this metric aptly displays the plausible magnitude of error term. It avoids the use of absolute error values which is highly undesirable in mathematical calculations. When we have more samples, reconstructing the error distribution using RMSE is considered to be more reliable. RMSE is highly affected by outlier values. Hence, make sure you’ve removed outliers from your data set prior to using this metric. As compared to mean absolute error, RMSE gives higher weightage and punishes large errors"
      ],
      "metadata": {
        "id": "4e2kBi_-55Xw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **We choose Random forest as our final prediction as it gives the best accuracy among all.**\n",
        "\n",
        "### Model Performance: The Goodness of fit determines how the line of regression fits the set of observations. The process of finding the best model out of various models is called optimization. It can be achieved by below method:\n",
        "\n",
        "### R-squared method:\n",
        "### R-squared is a statistical method that determines the goodness of fit. It measures the strength of the relationship between the dependent and independent variables on a scale of 0-100%. The high value of R-square determines the less difference between the predicted values and actual values and hence represents a good model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LIME ( Local Interpretable Model-agnostic Explanations )is a novel explanation technique that explains the prediction of any classifier in an interpretable and faithful manner by learning an interpretable model locally around the prediction.\n",
        "\n",
        "### A consistent model agnostic explainer .\n",
        "### A method to select a representative set with explanations to make sure the model behaves consistently while replicating human logic. This representative set would provide an intuitive global understanding of the model.\n",
        "### LIME explains a prediction so that even the non-experts could compare and improve on an untrustworthy model through feature engineering."
      ],
      "metadata": {
        "id": "SgLaeuws8H2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Model Explainability by LIME**\n"
      ],
      "metadata": {
        "id": "ayZu--Dq4kYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features\n",
        "float_columns=[]\n",
        "cat_columns=[]\n",
        "int_columns=[]"
      ],
      "metadata": {
        "id": "O0KXGPDsz8Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting features into respective float, cat , int list.\n",
        "for i in x.columns:\n",
        "    if x[i].dtype == 'float' : \n",
        "        float_columns.append(i)\n",
        "    elif x[i].dtype == 'int64':\n",
        "        int_columns.append(i)\n",
        "    elif x[i].dtype == 'object':\n",
        "        cat_columns.append(i)"
      ],
      "metadata": {
        "id": "OPwBjbrV0M-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat_features = x[cat_columns]\n",
        "train_float_features = x[float_columns]\n",
        "train_int_features = x[int_columns]"
      ],
      "metadata": {
        "id": "bIubssWr0QSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "5oDeUBR90Usv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "from __future__ import print_function"
      ],
      "metadata": {
        "id": "h0eO2MlH0Zj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Create the LIME Explainer\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(feature_names = X.columns,\n",
        "                                                  training_data = np.array(X_train),\n",
        "                                                  mode='regression')                                   "
      ],
      "metadata": {
        "id": "XV2OwtMe0d5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explanation for RandomForest\n",
        "exp = explainer.explain_instance(data_row = X_test.iloc[24], predict_fn = rf_model.predict)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "-DkPnCWF1nuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From LIME black box model, we can see that, Hour 4,Hour 5,Hour 3,Hour 2 and Rainfall has positive impact and Hour 18,Hour 20, Hour 8, Hour 19,Hour 21 has negative impact on rented bike count for Random Forest Model.\n",
        "\n"
      ],
      "metadata": {
        "id": "_VUE_6Ws1-r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explanation for Gradient Boosting with GridSearchCV\n",
        "exp = explainer.explain_instance(data_row = X_test.iloc[24], predict_fn = gb_grid.predict)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "t6xv3_I_2cfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From LIME black box model, we can see that, Hour 4,Hour 5, Rainfall,Hour 3,Hour 2 has positive impact and Hour 18,Hour 19, Hour 20, Hour 21,Hour 22 has negative impact on rented bike count for Gradient Boosting with GridSearchCV Model."
      ],
      "metadata": {
        "id": "KfPKKdfc3nOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the explanation for XGBoost with GridSearchCV\n",
        "exp = explainer.explain_instance(data_row = X_test.iloc[24], predict_fn = xgb_model.predict)\n",
        "exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "1tkWzF_x36gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From LIME black box model, we can see that, Hour 4,Hour 3,Hour 5, Rainfall,Hour 2 has positive impact and Hour 18,Hour 19, Hour 8,Hour 20  and Hour 21 has negative impact on rented bike count for XgBoost with GridSearchCV Model.\n",
        "\n"
      ],
      "metadata": {
        "id": "-EQp3oCG3_ph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### The number of bikes rented is on average higher during the rush hours.i.e. at 6 p.m. to 8 p.m.\n",
        "\n",
        "### The rented bike counts is higher during the summer and lowest during the winter.\n",
        "\n",
        "### The rented bike count is higher on working days than on non-working days.\n",
        "\n",
        "### On a non-functioning day, no bikes are rented in all the instances of the data.\n",
        "\n",
        "### The number of bikes rented on average remains constant throughout Monday - Saturday, it dips on Sunday, and on average, the rented bike counts is lower on weekends than on weekdays.\n",
        "\n",
        "### On regular days, the demand for the bikes is higher during rush hours. On holidays or weekends, the demand is comparatively lower in the mornings, and is higher in the afternoons."
      ],
      "metadata": {
        "id": "izkm3_Vt9mz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Results**"
      ],
      "metadata": {
        "id": "NFFish2y73XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I implemented 7 machine learning algorithms Linear Regression,lasso,ridge,elasticnet,decission tree, Random Forest and XGBoost. I have done  hyperparameter tuning to improve the model performance. The results of our evaluation are:\n",
        "\n"
      ],
      "metadata": {
        "id": "79KtqXhWCjHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the results of evaluation metric values for all models\n",
        "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "nfi87Aqhda-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No overfitting is seen.\n",
        "### Random forest Regressor, Gradient Boosting gridsearchcv and XgBoost Regressor with RandomisedSearchCV gives the highest R2 score of 98%, 95% and 96% respectively for Train Set and almost 92% for Test set.\n",
        "###  Feature Importance value for Random Forest, Gradient Boosting, XgBoost are different.\n",
        "### We can deploy Random Forest, Gradient Boosting with GridSearchCV, XgBoost with RandomizedSearchCV model.\n",
        " \n"
      ],
      "metadata": {
        "id": "v8BuE4wB8YME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### However, this is not the ultimate end. As this data is time dependent, the values for variables like temperature, windspeed, solar radiation etc., will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time. Therefore, having a quality knowledge and keeping pace with the ever evolving ML field would surely help one to stay a step ahead in future."
      ],
      "metadata": {
        "id": "J83GzODjCk0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}